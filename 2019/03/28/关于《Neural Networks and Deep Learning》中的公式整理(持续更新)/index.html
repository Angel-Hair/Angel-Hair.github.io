<!DOCTYPE html>
<html lang="zh-Hans">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Angel Hair">


    <meta name="subtitle" content="绿水青山在 留得个柴烧">


    <meta name="description" content="_(:зゝ∠)_">



<title>关于《Neural Networks and Deep Learning》中的公式整理(持续更新) | Cat Room</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Cat Room</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Cat Room</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">关于《Neural Networks and Deep Learning》中的公式整理(持续更新)</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Angel Hair</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 28, 2019&nbsp;&nbsp;19:09:12</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在看Michael Nielsen的《Neural Networks and Deep Learning》，想整理下里面的一些公式帮助记忆，顺便还可以随取随用。</p>
<p>注意:正文章节按照原文章节整理分布，并且我有意没有删掉公式序号，方便查找。</p>
<p>另外，根据网上推荐，我也在coursera上追吴恩达博士的机器学习课，感觉很好，推荐给大家。</p>
<h1 id="公式整理"><a href="#公式整理" class="headerlink" title="公式整理"></a>公式整理</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机工作原理:</p>
<p>$$\begin{eqnarray} \mbox{output} &amp; = &amp; \left{ \begin{array}{ll} 0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \ 1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}$$</p>
<p>$$\begin{eqnarray} \mbox{output} = \left{ \begin{array}{ll} 0 &amp; \mbox{if } w\cdot x + b \leq 0 \ 1 &amp; \mbox{if } w\cdot x + b &gt; 0 \end{array} \right. \tag{2}\end{eqnarray}$$</p>
<h2 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h2><p>sigmoid函数(sigmoid function):</p>
<p>$$\begin{eqnarray} \sigma(z) \equiv \frac{1}{1+e^{-z}} \equiv \frac{1}{1+\exp(-\sum_j w_j x_j-b)}. \tag{3}\end{eqnarray}$$</p>
<p>关于σ函数的平滑属性的神经元输出改变量:</p>
<p>$$\begin{eqnarray} \Delta \mbox{output} \approx \sum_j \frac{\partial , \mbox{output}}{\partial w_j} \Delta w_j + \frac{\partial , \mbox{output}}{\partial b} \Delta b, \tag{5}\end{eqnarray}$$</p>
<h2 id="通过梯度下降法学习参数"><a href="#通过梯度下降法学习参数" class="headerlink" title="通过梯度下降法学习参数"></a>通过梯度下降法学习参数</h2><p>平方代价函数/均方误差/MSE:</p>
<p>$$\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2. \tag{6}\end{eqnarray}$$</p>
<p>其他形式有:</p>
<p>$$\begin{eqnarray} C = \frac{1}{2n} \sum_x |y(x)-a^L(x)|^2, \tag{26}\end{eqnarray}$$</p>
<p>速度变化量:</p>
<p>$$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$$</p>
<p>梯度向量:</p>
<p>$$\begin{eqnarray} \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T. \tag{8}\end{eqnarray}$$</p>
<p>代入学习率的速度变化量:</p>
<p>$$\begin{eqnarray} \Delta v = -\eta \nabla C, \tag{10}\end{eqnarray}$$</p>
<p>代价函数变化量:</p>
<p>$$\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \approx \nabla C \cdot \Delta v = -\eta |\nabla C|^2. \tag{7、9}\end{eqnarray}$$</p>
<p>$v$移动公式:</p>
<p>$$\begin{eqnarray} v \rightarrow v’ = v -\eta \nabla C. \tag{11}\end{eqnarray}$$</p>
<p>神经网络中的梯度下降:</p>
<p>$$\begin{eqnarray} w_k &amp; \rightarrow &amp; w_k’ = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\ b_l &amp; \rightarrow &amp; b_l’ = b_l-\eta \frac{\partial C}{\partial b_l}. \tag{17}\end{eqnarray}$$</p>
<p>随机梯度下降（SGD）中的平均 $\nabla C_x$ :</p>
<p>$$\begin{eqnarray} \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C \tag{18}\end{eqnarray}$$</p>
<p>SGD中的梯度下降:</p>
<p>$$\begin{eqnarray} w_k &amp; \rightarrow &amp; w_k’ = w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\ b_l &amp; \rightarrow &amp; b_l’ = b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}  \tag{21}\end{eqnarray}$$</p>
<h2 id="一个基于矩阵的快速计算神经网络输出的方法"><a href="#一个基于矩阵的快速计算神经网络输出的方法" class="headerlink" title="一个基于矩阵的快速计算神经网络输出的方法"></a>一个基于矩阵的快速计算神经网络输出的方法</h2><p>从第$l−1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接的权重: $w_{jk}^l$</p>
<p>第$l$层第$j$个神经元的偏置: $b^l_j$</p>
<p>第$l$层的第$j$个神经元的激活值: $a^l_j$</p>
<p>激活值 $a^l_j$ 公式:</p>
<p>$$\begin{eqnarray} a^{l}<em>j = \sigma\left( \sum_k w^{l}</em>{jk} a^{l-1}_k + b^l_j \right) \tag{23}\end{eqnarray}$$</p>
<p>其矩阵形式为:</p>
<p>$$\begin{eqnarray} a^{l} = \sigma(w^l a^{l-1}+b^l).\tag{25}\end{eqnarray}$$</p>
<p>第$l$层神经元的加权输入(weighted input):</p>
<p>$$z^l_j= \sum_k w^l_{jk} a^{l-1}_k+b^l_j$$</p>
<p>其矩阵形式:</p>
<p>$$z^l \equiv w^l a^{l-1}+b^l$$</p>
<h2 id="关于代价函数的两个假设"><a href="#关于代价函数的两个假设" class="headerlink" title="关于代价函数的两个假设"></a>关于代价函数的两个假设</h2><p>代价函数的第一假设形式:</p>
<p>$$C=\frac{1}{n} \sum_x C_x$$</p>
<p>第二假设中单一训练样本$x$的二次代价:</p>
<p>$$\begin{eqnarray} C_x = \frac{1}{2} |y-a^L|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2 \tag{27}\end{eqnarray}$$</p>
<h2 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h2><p>$$(s \odot t)_j = s_j t_j$$</p>
<h2 id="反向传播背后的四个基本等式"><a href="#反向传播背后的四个基本等式" class="headerlink" title="反向传播背后的四个基本等式"></a>反向传播背后的四个基本等式</h2><p>$l$层的$j$神经元的错误量(error):$\delta^l_j$</p>
<p>错误量定义:</p>
<p>$$\begin{eqnarray} \delta^l_j \equiv \frac{\partial C}{\partial z^l_j} \tag{29}\end{eqnarray}$$</p>
<p>输出层中关于错误量$\delta^L$的等式(BP1):</p>
<p>$$\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j) \tag{BP1}\end{eqnarray}$$</p>
<p>BP1的矩阵形式:</p>
<p>$$\begin{eqnarray} \delta^L = (a^L-y) \odot \sigma’(z^L). \tag{30}\end{eqnarray}$$</p>
<p>依据下一层错误量$\delta^{l+1}$获取错误量$\delta^l$的矩阵形式(BP2):</p>
<p>$$\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l) \tag{BP2}\end{eqnarray}$$</p>
<p>网络的代价函数相对于偏置的改变速率的等式(BP3):</p>
<p>$$\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j \tag{BP3}\end{eqnarray}$$</p>
<p>BP3的简略式:</p>
<p>$$\begin{eqnarray} \frac{\partial C}{\partial b} = \delta \tag{31}\end{eqnarray}$$</p>
<p>网络的代价函数相对于权重的改变速率的等式(BP4):</p>
<p>$$\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \tag{BP4}\end{eqnarray}$$</p>
<p>BP4的简略式:</p>
<p>$$\begin{eqnarray} \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out} \tag{32}\end{eqnarray}$$</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>说来也奇怪，我在追吴恩达博士的课的时候，学到吴恩达介绍Yann LeCun的一段video的那段视频，刚好是公布Yann LeCun荣获2018年图灵奖开始的时候。</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Angel Hair</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"># 读书笔记</a>
                    
                        <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"># 神经网络</a>
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"># 深度学习</a>
                    
                        <a href="/tags/%E3%80%8ANeural-Networks-and-Deep-Learning%E3%80%8B/"># 《Neural Networks and Deep Learning》</a>
                    
                        <a href="/tags/%E5%85%AC%E5%BC%8F/"># 公式</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/01/04/20200103.log/">20200103.log</a>
            
            
            <a class="next" rel="next" href="/2019/02/01/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%85%8B%E8%8E%B1%E5%B0%94%C2%B7%E8%8E%B1%E8%BE%9B%E7%9A%84%E5%9D%9A%E6%8C%81/">读书笔记——克莱尔·莱辛的坚持</a>
            
        </section>


    </article>
</div>


    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '50a78039a642b3e969cf',
        clientSecret: 'd747252c944bf94910270439687c98575f0b6f4f',
        repo: 'Gitalk_comments',
        owner: 'Angel-Hair',
        admin: 'Angel-Hair',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 8,
        pagerDirection: 'last',
        createIssueManually: true,
        distractionFreeMode: false
    })
    gitalk.render('gitalk-container')      
</script>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Angel Hair | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
